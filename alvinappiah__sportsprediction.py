# -*- coding: utf-8 -*-
"""AlvinAppiah._SportsPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i1bjMaRaLVrVBMKURpp_NgWds5PSunUc
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

"""# Question 1"""

fifa = pd.read_csv("/content/drive/MyDrive/FIFA 2023/male_players (legacy).csv")

threshold = int(0.3 * 100)
new_fifa = fifa.dropna(thresh = threshold)

numeric_data= new_fifa.select_dtypes(include=['int64','float64'])

#drops goalkeeping column
new_fifa = new_fifa.drop(columns = ['goalkeeping_speed'])

#Separates data into only integers and floats
numeric_data= new_fifa.select_dtypes(include=['int64','float64'])

#Separates data into only strings
non_numeric = new_fifa.select_dtypes(include = ['object'])

#drops unessecary columns
new_fifa = new_fifa.drop(columns = ['club_loaned_from', 'player_url', 'fifa_update_date', 'player_face_url', 'real_face', 'short_name', 'dob'])

from sklearn.impute import SimpleImputer

sc = SimpleImputer(strategy='median')

#Fills all nas in the numeric data with the median values
numeric_data_filled = sc.fit_transform(numeric_data)

#Creates a dataframe of the data with the filled values
numeric_data_df = pd.DataFrame(numeric_data_filled, columns=numeric_data.columns)

sc1 = SimpleImputer(strategy='most_frequent')

#fills all nas with the most frequent values
non_numeric_filled = sc1.fit_transform(non_numeric)

#creates a dataframe of data with filled values
non_numeric_df = pd.DataFrame(non_numeric_filled, columns=non_numeric.columns)

# Adds the dataframes after filling the nas
Xfilled = pd.concat([non_numeric_df,numeric_data_df], axis = 1)

"""# Question 2"""

from sklearn.preprocessing import LabelEncoder, OneHotEncoder

categorical_list = non_numeric.columns.tolist()

#encodes all the strings in the data
def encode(column_list):
    label_encoder = LabelEncoder()
    integer_encoded = label_encoder.fit_transform(column_list)
    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)

    return integer_encoded

#encodes all strings
for i in categorical_list:
  Xfilled[i] = Xfilled[i].astype(str)
  Xfilled[i] = Xfilled[i].tolist()
  Xfilled[i] = encode(Xfilled[i])

corr_matrix = Xfilled.corr()

correlations = corr_matrix['overall'].sort_values(ascending=False)

#Creates a file with all the correlations
correlations.to_csv('correlations')

"""**The correlation cutoff for this model is 0.5. Since the variables do not have have a strong negative correlation,**
**I will be using 10 features with a correlation greater than or equal 0.5 when rounded up. The variables being used are**
* movement_reactions
* potential
* passing
* wage_eur
* mentality_composure
* value_eur
* dribbling
* Attacking_short_passing
* mentality_vision
* international_reputation
"""

x = Xfilled[['movement_reactions','potential','passing','wage_eur',
             'value_eur','dribbling','attacking_short_passing','mentality_composure',
             'mentality_vision', 'international_reputation']]

y = Xfilled['overall']

from sklearn.model_selection import train_test_split, cross_val_score, KFold

n_folds = 10
kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)

#random_state = Means we want to randomize the data
Xtrain,Xtest,Ytrain,Ytest = train_test_split(x,y,test_size=0.2, random_state=42)

"""# Question 3

## XGB Regressor
"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score

from sklearn.model_selection import GridSearchCV

# xgb regressor to train model
import xgboost as xgb
model1 = xgb.XGBRegressor()
model1_scores = cross_val_score(model1, Xtrain, Ytrain, cv=kf, scoring='neg_mean_squared_error')
model1.fit(Xtrain, Ytrain)
xgb_test_score = model1.score(Xtest, Ytest)
y_pred = model1.predict(Xtest)
print(f"""
Mean Absolute Error = {mean_absolute_error(y_pred,Ytest)},
Mean Squared Error = {mean_squared_error(y_pred,Ytest)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred,Ytest))},
R2 Score = {r2_score(y_pred, Ytest)}
          """)
print(f"Test: {xgb_test_score}")

# gradient boosting regressor to train model
from sklearn.ensemble import GradientBoostingRegressor
model2 = GradientBoostingRegressor()
model2_scores = cross_val_score(model2, Xtrain, Ytrain, cv=kf, scoring='neg_mean_squared_error')
model2.fit(Xtrain, Ytrain)
grad_test_score = model2.score(Xtest, Ytest)
y_pred2 = model2.predict(Xtest)
print(f"""
Mean Absolute Error = {mean_absolute_error(y_pred2,Ytest)},
Mean Squared Error = {mean_squared_error(y_pred2,Ytest)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred2,Ytest))},
R2 Score = {r2_score(y_pred2, Ytest)}
          """)
print(f"Test: {grad_test_score}")

"""## Grid Search"""

parameters = {
    'gb__n_estimators': [100, 200],
    'gb__max_depth': [3, 4, 5],
    'gb__learning_rate': [0.01, 0.1],
}
grid_search = GridSearchCV(estimator = xgb.XGBRegressor(), param_grid = parameters, cv=5, scoring='neg_mean_squared_error')

#Fit the grid search on the training data
grid_search.fit(Xtrain, Ytrain)
#Gets the best parameters
best_params = grid_search.best_params_
best_score = np.sqrt(-grid_search.best_score_)

print("Best Parameters:", best_params)
print("Best CV RMSE: %.4f" % best_score)
#gets the best model
best_model = grid_search.best_estimator_
stack_y_pred = best_model.predict(Xtest)
stack_test_score = best_model.score(Xtest, Ytest)

print(f"Test: {stack_test_score}")
print(f"""
Mean Absolute Error = {mean_absolute_error(stack_y_pred, Ytest)},
Mean Squared Error = {mean_squared_error(stack_y_pred, Ytest)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(stack_y_pred, Ytest))},
R2 Score = {r2_score(stack_y_pred, Ytest)}
""")

print(f"Test Score: {stack_test_score}")

#Stacking regressor to combine all the regressors used
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import LinearRegression
model4 = LinearRegression()
all_models = [('gb', model2), ('lr', model4)]
meta_model = LinearRegression()
stacking_model = StackingRegressor(estimators=all_models, final_estimator=meta_model, cv=5)
stacking_model.fit(Xtrain,Ytrain)
stack_test_score = stacking_model.score(Xtest, Ytest)
stack_y_pred = stacking_model.predict(Xtest)

print(f"""
Mean Absolute Error = {mean_absolute_error(stack_y_pred,Ytest)},
Mean Squared Error = {mean_squared_error(stack_y_pred,Ytest)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(stack_y_pred,Ytest))},
R2 Score = {r2_score(stack_y_pred, Ytest)}
          """)
print(f"Test Score: {stack_test_score}")

"""# Question 5"""

player_22 = pd.read_csv('/content/drive/MyDrive/FIFA 2023/players_22.csv')

testX = player_22[['movement_reactions','potential','passing','wage_eur',
             'value_eur','dribbling','attacking_short_passing','mentality_composure',
             'mentality_vision', 'international_reputation']]

testy =  player_22[['overall']]

test_numeric_data= testX.select_dtypes(include=['int64','float64'])

sc_test = SimpleImputer(strategy='median')
test_numeric_data_filled = sc_test.fit_transform(test_numeric_data)

test_numeric_data_df = pd.DataFrame(test_numeric_data_filled, columns=test_numeric_data.columns)

testX = test_numeric_data_df

testy = player_22[['overall']]

a = pd.concat([testy,testX], axis = 1)

Xtrain1,Xtest1,Ytrain1,Ytest1 = train_test_split(testX,testy,test_size=0.2, random_state=42)

best_model

test_y_pred = model1.predict(Xtest1)
print(f"""
Mean Absolute Error = {mean_absolute_error(Ytest1, test_y_pred)},
Mean Squared Error = {mean_squared_error(Ytest1, test_y_pred)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(Ytest1, test_y_pred))},
R2 Score = {r2_score(Ytest1, test_y_pred)}
          """)

import joblib
joblib.dump(best_model, 'best_performing_model.pkl', compress = 9)

